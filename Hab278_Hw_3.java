import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class Hab278_Hw_3 {  public static void main(String[] args) throws Exception {    if (args.length != 2) {      System.err.println("Usage: Hab278Hw3 <input path> <output path>");      System.exit(-1);    }    String inputPath = args[0];    String outputPath = args[1];    String originalOutputPath = outputPath;    Job job;    for(int i = 0; i < 3; i++ )    {      job = new Job();      job.setJarByClass(Hab278_Hw_3.class);      job.setJobName("Hab278_Hw_3");      FileInputFormat.addInputPath(job, new Path(inputPath));      FileOutputFormat.setOutputPath(job, new Path(outputPath));      job.setMapperClass(Hab278_Hw_3_Mapper.class);      job.setReducerClass(Hab278_Hw_3_Reducer.class);      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(Text.class);      inputPath = outputPath + "/part-r-00000";      outputPath = originalOutputPath + (i + 1);      if(!job.waitForCompletion(true))        System.exit(1);    }  }}